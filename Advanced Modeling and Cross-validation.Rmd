---
title: 'Advanced Modeling and Cross-validation'
subtitle: 
geometry: margin = 2.25cm
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

__Situation:__ Can we predict the selling price of a house in Ames, Iowa based on recorded features of the house? That is your task for this assignment. Each team will get a dataset with information on forty potential predictors and the selling price (in $1,000’s) for a sample of homes.

```{r}
library(readr)
library(dplyr)

AmesHouse <- read_csv("AmesTrain10.csv")
AmesHouseTest <- read_csv("AmesTest10.csv")
```

```{r}
head(AmesHouseTest)
```


#### Part 6. Cross-validation: ####
In some situations, a model might fit the peculiarities of a specific sample of data well, but not reflect structure that is really present in the population. A good test for how your model might work on "real" house prices can be simulated by seeing how well your fitted model does at predicting prices that were NOT in your original sample. This is why we reserved an additional 200 cases as a holdout sample in AmesTest10.csv. 

* Compute the predicted Price for each of the cases in the holdout test sample, using your model resulting from the initial fit and residual analysis from "Model Selection Methods" Github Page

```{r}
AmesHouse_Numeric = AmesHouse %>% select_if(is.numeric)

AmesHouse_Numeric_po <- AmesHouse_Numeric[-c(179),]

step_mod_po = lm(Price ~ Quality + FirstSF + SecondSF + BasementFinSF + 
    LotArea + YearBuilt + GarageSF + BasementSF + YearRemodel + 
    LotFrontage + Fireplaces + HalfBath + Condition + Bedroom + 
    TotalRooms + ScreenPorchSF + EnclosedPorchSF, data = AmesHouse_Numeric_po)

AmesHouseTest_Numeric = AmesHouseTest %>% select_if(is.numeric)

fitprice=predict(step_mod_po,newdata=AmesHouseTest_Numeric)

```


* Compute the residuals for the 200 holdout cases.

```{r}
holdoutresid=AmesHouseTest_Numeric$Price - fitprice
head(holdoutresid)
```


* Compute the mean and standard deviation of these residuals. Are they close to what you expect from the training model?


```{r}
mean(holdoutresid)
sd(holdoutresid)
summary(step_mod_po)$sigma
```

These are values that we would expect from the training model. The means is close to zero considering how large the price values are. The standard deviations are fairly close, but this is also dependent on what we consider "close."

* Construct a plot of the residuals to determine if they are normally distributed. Is this plot what you expect to see considering the training model?

```{r}
qqnorm(holdoutresid)
qqline(holdoutresid)
```

These residuals are normaly distributed for the most part. There are two values that are huge outliers, but other than that I would say that it closely follows the normal Q-Q plot line considering the large scale that price has.

* Are any holdout cases especially poorly predicted by the training model? If so, identify by the row number(s) in the holdout data. 

```{r}
head(sort(holdoutresid, decreasing=TRUE), 10)
head(sort(holdoutresid), 10)
```

I have picked out the top 10 positive and top 10 negative residuals. Rows 48 and 118 have particularly poorly predicted prices.

* Compute the correlation between the predicted values and actual prices for the holdout sample. This is known as the cross-validation correlation. We don’t expect the training model to do better at predicting values different from those that were used to build it (as reflected in the original $R^{2}$), but an effective model shouldn’t do a lot worse at predicting the holdout values. Square the cross-validation correlation to get an $R^{2}$ value and subtract it from the original multiple $R^{2}$ of the training sample. This is known as the shrinkage. We won’t have specific rules about how little the shrinkage should be, but give an opinion on whether the shrinkage looks OK to you or too large in your situation. 

```{r}
crosscorr=cor(AmesHouseTest_Numeric$Price,fitprice)
crosscorr^2

shrinkage = summary(step_mod_po)$r.squared-crosscorr^2
shrinkage
```

The shrinkage does look OK to me. It is not too large considering about a 0.076 value.

#### Part 7. Find a 'fancy model': ####

In this section, we decided we should start by using the three methods to create a model (forward, backward, stepwise) to get an idea of what our model should look like before transforming it.

```{r}
AmesTrain10 <- AmesHouse
Full=lm(Price~.,data=AmesTrain10[,2:42])
none=lm(Price~1,data=AmesTrain10[,2:42])
MSE=(summary(Full)$sigma)^2
```

```{r}
#Forward Test
step(none,scope=list(upper=Full), scale=MSE,direction="forward", trace=FALSE)
```

```{r}
Forward_Test = lm(formula = Price ~ Quality + FirstSF + SecondSF + ExteriorQ +
BasementFinSF + LotArea + BasementHt + YearBuilt + Condition + BasementSF + GarageSF + Fireplaces + Foundation + LotFrontage + FullBath + Heating + EnclosedPorchSF + GarageType, data = AmesTrain10[, 2:42])
summary(Forward_Test)
```

One thing we noticed with the forward test is that it used FirstSF and SecondSF but there was no use of GroundSF so I wanted to take those out and replace it with Ground SF to see if that improves the R^2 value at all.

```{r}
Forward_Test2 = lm(formula = Price ~ Quality + GroundSF + ExteriorQ + BasementFinSF + LotArea + BasementHt + YearBuilt + Condition + BasementSF + GarageSF + Fireplaces + Foundation + LotFrontage + FullBath + Heating + EnclosedPorchSF + GarageType, data = AmesTrain10[,2:42])
summary(Forward_Test2)
```

After replacing it with GroundSF, the R^2 value was increased by a little bit but not by a large amount. 

```{r}
#Backward Test:
step(Full,scale=MSE, trace=FALSE)
```

```{r}
Backward_Test = lm(Price ~ LotFrontage + LotArea + HouseStyle + Quality + Condition + YearBuilt + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + SecondSF + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + GarageSF + EnclosedPorchSF, data = AmesTrain10[, 2:42])

summary(Backward_Test)
```

With the backward test, we saw that it included the SecondSF and GroundSF but not FirstSF so we wanted to see if removing SecondSF would have an effect on the model as well as factoring Quality and Condition to treat them as categorical variables.

```{r}
Backward_Test2 = lm(Price ~ LotFrontage + LotArea + HouseStyle + factor(Quality) + factor(Condition) + YearBuilt + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + GarageSF + EnclosedPorchSF, data = AmesTrain10[, 2:42])

summary(Backward_Test2)
```


We notice that the R^2 increased by a little when removing the SecondSF value and factoring both Quality and Condition.

```{r}
#Stewpise Test
step(none,scope=list(upper=Full),scale=MSE, trace=FALSE)
```


```{r}
Stepwise_Test = lm(Price ~ Quality + FirstSF + SecondSF + ExteriorQ + BasementFinSF + LotArea + BasementHt + YearBuilt + Condition + BasementSF + GarageSF + Fireplaces + Foundation + LotFrontage + FullBath + Heating + EnclosedPorchSF + GarageType, data = AmesTrain10[,2:42])

summary(Stepwise_Test)
```

Again, we see that there is FirstSF and SecondSF but no GroundSF so we will try another test to see if having GroundSF could possibly improve our model.

```{r}
Stepwise_Test2 = lm(Price ~ Quality + GroundSF + ExteriorQ + BasementFinSF + LotArea + BasementHt + YearBuilt + Condition + BasementSF + GarageSF + Fireplaces + Foundation +LotFrontage +
FullBath + Heating + EnclosedPorchSF + GarageType, data = AmesTrain10[,2:42])
summary(Stepwise_Test2)
```


After replacing FirstSF and SecondSF with GroundSF, we see that the adjusted R^2 value increased by a little as it went fom .9043 to 9045, indicating slight improvement within the model.

For our fancy model, we will add in the factor function on Quality and Condition as we want these to be interpreted as categorical variables and we will also take out FirstSF and SecondSF and replace it with GroundSF.

```{r}
FancyModel= lm(Price ~ LotFrontage + LotArea + HouseStyle + factor(Quality) + factor(Condition) + YearBuilt + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + GroundSF + FullBath +
HalfBath + Fireplaces + GarageType + GarageSF + EnclosedPorchSF,
    data = AmesTrain10[, 2:42])
summary(FancyModel)
```


Having this given model, we see that our R^2 value has increased and instead of being around .9043 we are now .9134!
Next, we want to remove points that could possibly be outliers and effect our model in the long run.

```{r}
head(sort(rstandard(FancyModel), decreasing = TRUE), n = 10)
head(sort(rstudent(FancyModel), decreasing = TRUE), n = 10)
head(sort(cooks.distance(FancyModel), decreasing = TRUE), n = 10)
```

We did this by comparing the standardized values with the studentized values and saw if they were similar and also if they were above 2-3 Standard Deviations. Furthermore, we also looked at the hat-values and decided to take anything out that was between 0.5-1.

```{r}
AmesTrain10[c(343,109,222, 179),]

AmesTrain10_reduced = subset(AmesTrain10, Order != 45 & Order != 1641 & Order != 1638 & Order != 2116)
```

Then we take those given data points out and have a new reduced model that will have possibly taken out the higher residual points with outliers.

From previous modeling in the github page "Model Selection Methods", we saw that it would be best to log(Price) when given the chance to transform the model to better fit our data to deal with possible curvature and skewness.

```{r}
LogFancyModel = lm(log(Price) ~ LotFrontage + LotArea + HouseStyle +
factor(Quality) +
factor(Condition) + YearBuilt + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + GroundSF + FullBath +
HalfBath + Fireplaces + GarageType + GarageSF + EnclosedPorchSF,
data = AmesTrain10_reduced[, 2:42])
summary(LogFancyModel)
```

Now, our R^2 value increased a little more to become .9149!

Next, we will plot each of the predictors by price to see which ones might need specific modifications in order to better fit the data.

```{r}
plot(Price ~ LotFrontage + LotArea + YearBuilt + BasementFinSF + BasementSF + GroundSF + FullBath + HalfBath + Fireplaces + GarageSF + EnclosedPorchSF,data=AmesTrain10_reduced)
```

After doing so, we see how each of our quantitative predictors are set in the graph and go about trying different transformations to better fit our data and we can even add Interactions if we will feel that may best fit the data.

```{r}
TransFancyMod1 = lm(log(Price) ~ LotFrontage + I(sqrt(LotArea)) + HouseStyle + factor(Quality) + factor(Condition) + I(YearBuilt^2) + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + I(GarageSF^2) + EnclosedPorchSF, data = AmesTrain10_reduced[, 2:42])
plot(TransFancyMod1)
```

```{r}
summary(TransFancyMod1) #R^2 squared value = .9219
```

```{r}
TransFancyMod2 = lm(log(Price) ~ LotFrontage + I(LotArea^0.5) + HouseStyle + factor(Quality) + factor(Condition) + I(YearBuilt^3) + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + I(GarageSF^3) + EnclosedPorchSF, data = AmesTrain10_reduced[, 2:42])
plot(TransFancyMod2)
```


```{r}
summary(TransFancyMod2) #R^2 squared value = .9216
```

```{r}
TransFancyMod3 = lm(log(Price) ~ LotFrontage + I(LotArea^0.5) + HouseStyle + factor(Quality) + factor(Condition) + I(YearBuilt^4) + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + I(GarageSF^4) + EnclosedPorchSF, data = AmesTrain10_reduced[, 2:42])
plot(TransFancyMod3)
summary(TransFancyMod3) #R^2 squared value = .9213
```

Now add interactions within the the predictors to see if it will be a better fit for the model.

```{r}
FinalModel = lm(log(Price) ~ LotFrontage + I(sqrt(LotArea)) + I(LotFrontage * sqrt(LotArea)) + HouseStyle + factor(Quality) + factor(Condition) + I(YearBuilt^2) + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + I(BasementFinSF * BasementSF) + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + I(GarageSF^2) + EnclosedPorchSF, data =
AmesTrain10_reduced[, 2:42])
plot(FinalModel)
```

```{r}
summary(FinalModel)
```

Now we need to do a residual analysis for the given model... 

Linearity: The linearity has obviously improved based upon the original model. There is little to no curve shown by the red line in the residuals vs. fitted plot. 

Constant Variance: Constant variance does not show any large issues. The variance on the left side of the residuals vs. fitted plot may be a little more wide, but for the most part this condition is met. 

Normality: The normality of the residuals looks much better as well. The right skew is little to none. The left skew is a little bit larger, but we know that some skew on the ends is going to occur. This condition is also met.


#### Part 8. Cross Validation for Fancy Model #### 

```{r}
AmesTest10 = AmesHouseTest
summary(FinalModel)

AmesTest10$BasementHt[AmesTest10$BasementHt == "0"]="None"
fitPrice2 = exp(predict(FinalModel, newdata = AmesTest10)) #check this to untransform
head(fitPrice2)

holdoutresid2=AmesTest10$Price - fitPrice2
head(holdoutresid2)
mean(holdoutresid2)
sd(holdoutresid2)
```


The mean of the residuals is close to 0. The value is 0.5, which is very low for a price. However, the standard deviation value is far off of the original model. The standard deviation of the holdout residuals is 27.28, which is very far off from the original model residual standard error of 0.1189. However,the standard deviation value of 33.41682 is pretty closely aligned to the training model standard deviation of 33.57.

```{r}
crosscorr = cor(AmesTest10$Price, fitPrice2)
crosscorr

crosscorr^2

shrinkage = summary(FinalModel)$r.squared-crosscorr^2
shrinkage
```

The normal q-q plot shows that most of these values run along the line, but there are about 15 or so values that seem to be far from normal. There is a larger skew on the right than the left. The cross correlation is large, which means that this model possibly fits this data very well. The shrinkage also supports this since it is 0.058. This value is not large enough to say that this model is not fit for the test data.



#### Part 9. Final Model ####  
    
Suppose that you are interested in a house in Ames, Iowa that has characteristics listed below and want to find a 95% prediction interval for the price of this house.     

A 2 story 11 room home, built in 1983 and remodeled in 1999 on a 21540 sq. ft. lot with 400 feet of road frontage. Overall quality is good (7) and condition is average (5). The quality and condition of the exterior are both good (Gd) and it has a poured concrete foundation. There is an 757 sq. foot basement that has excellent height, but is completely unfinished and has no bath facilities. Heating comes from a gas air furnace that is in excellent condition and there is central air conditioning. The house has 2432 sq. ft. of living space above ground, 1485 on the first floor and 947 on the second, with 4 bedrooms, 2 full and one half baths, and 1 fireplace. The 2 car, built-in garage has 588 sq. ft. of space and is average (TA) for both quality and construction. The only porches or decks is a 384 sq. ft. open porch in the front.


```{r}
AmesTrain10_reduced = subset(AmesHouse, Order != 45 & Order != 1641 & Order != 1307 & Order != 131 & Order != 182 & Order != 2116 & Order != 1638)

FinalModel = lm(log(Price) ~ LotFrontage + I(sqrt(LotArea)) + I(LotFrontage * sqrt(LotArea)) + HouseStyle + factor(Quality) + factor(Condition) + I(YearBuilt^2) + ExteriorQ + Foundation + BasementHt + BasementFinSF + BasementSF + I(BasementFinSF * BasementSF) + GroundSF + FullBath + HalfBath + Fireplaces + GarageType + I(GarageSF^2) + EnclosedPorchSF, data = AmesTrain10_reduced[, 2:42])
plot(FinalModel)
summary(FinalModel)
```


```{r}
newx=data.frame(
  HouseStyle = "2Story", 
  TotalRooms=11, 
  YearBuilt=1983, 
  YearRemodel=1999, 
  LotArea=21540, 
  #Lotconfig =, 
  LotFrontage=400, 
  Quality=7, 
  Condition=5, 
  ExteriorQ="Gd", 
  ExteriorC="Gd", 
  Foundation="PConc", 
  BasementHt="Ex", 
  BasementFin = "Unf",
  BasementFinSF = 0,
  BasementUnFinSF = 757, 
  BasementSF = 757, 
  BasementFBath=0, 
  BasementHBath=0, 
  Heating="GasA", 
  HeatingQC="Ex", 
  CentralAir="Y", 
  GroundSF=2432, 
  FirstSF=1485, 
  SecondSF=947, 
  Bedroom=4, 
  FullBath=2, 
  HalfBath=1, 
  Fireplaces=1, 
  GarageType="BuiltIn", 
  GarageCars=2, 
  GarageSF=588, 
  GarageQ="TA", 
  GarageC="TA", 
  OpenPorchSF=384, 
  EnclosedPorchSF = 0, 
  ScreenPorchSF=0)
head(newx)
#untransform
exp(predict.lm(FinalModel, newx, interval="prediction", level = 0.95))
```
The 95% prediction interval for the price of this house is between $183,680 and $361,914.
